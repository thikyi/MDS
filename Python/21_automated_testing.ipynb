{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a3f99ce-2ca4-4de3-acb1-dee2af24158e",
   "metadata": {},
   "source": [
    "# Assignment 21: Automated Testing #\n",
    "\n",
    "### Goals for this Assignment ###\n",
    "\n",
    "By the time you have completed this assignment, you should be able to:\n",
    "\n",
    "- Define _unit tests_ and a _test suite_ using Python's `unittest` library\n",
    "- Use `assertEqual` to ensure that two values are equal to each other inside of a unit test\n",
    "\n",
    "## Step 1: Define Unit Tests over a `my_min` Function ##\n",
    "\n",
    "### Background: Automated Testing ###\n",
    "\n",
    "For most steps in most of the assignments so far, you have needed to (in order):\n",
    "\n",
    "1. Write some code\n",
    "2. Run said code\n",
    "3. Ensure that the output of the said code was correct, and go back to the first step if not\n",
    "\n",
    "With respect to step 3, if there are only small amounts of output to verify correct, this likely didn't take very long.\n",
    "However, if there are large amounts of output to check, this can quickly get overwhelming, especially if you need to do any sort of debugging and iteration back to the first step.\n",
    "For example, if you are writing a function `foo`, and you need to check the outputs of 50 separate calls to `foo`, this will likely take a decent amount of time.\n",
    "If you discover that the 50th line of output was wrong, you now need to iterate back on the code, and debug it.\n",
    "\n",
    "Let's say you are reasonably sure you've fixed the bug.\n",
    "This means you now need to run the code again, and verify the output again.\n",
    "This checking of outputs starts to seem a bit tedious.\n",
    "Plus, I mean, it _just_ worked correctly for the prior 49 outputs right?\n",
    "So certainly it should be ok just to check the last line of output which was wrong before, right?\n",
    "\n",
    "Unfortunately, wrong.\n",
    "You may have fixed one bug but introduced another, in which case your last line of output might be correct, but one of those prior lines are now incorrect.\n",
    "This is referred to as a [regression bug](https://en.wikipedia.org/wiki/Software_regression), and they are surprisingly easy (and common) to introduce.\n",
    "The presence of one bug can mask the presence of another bug, and make code _appear_ to work correctly when it, in fact, doesn't.\n",
    "All this said, you unfortunately _do_ need to check all 50 lines of output again.\n",
    "\n",
    "Now, enter another problem: humans are just plain not very good at handling large volumes of data.\n",
    "\"Large\" in this context is unfortunately not even particularly large.\n",
    "With 50 lines of output to check, it's very easy for our eyes to glaze over and miss something, especially if you need to do this check multiple times.\n",
    "Our brains just plain aren't good for repetitive, data-oriented tasks like this, and mistakes are inevitable no matter how careful you're being.\n",
    "In fact, our brains are [quick to ignore even alarms, if they are too repetitive](https://en.wikipedia.org/wiki/Alarm_fatigue).\n",
    "\n",
    "> Case in point, there is an error in the syllabus which didn't get caught until shortly before this writing: the suggested readings for week 8, as of this writing, say \"Enter information here\", which came from a syllabus template.\n",
    "> I eventually decided there would not be any suggested readings for week 8.\n",
    "> I removed this from one location I was internally managing, but I neglected to remove this from the syllabus document.\n",
    "> This was missed by me and at least two other people, likely more, despite the fact that all parties reviewed multiple versions of the syllabus which had the flaw.\n",
    "> How did we all miss it?\n",
    "> There were 8 weeks of very similarly-looking content spanning four pages, and this was right at the end.\n",
    "> Furthermore, there were other issues that _did_ get caught before this, which likely caused fatigue to set in.\n",
    "\n",
    "While humans aren't good at tediously processing large volumes of data, computers excel at it.\n",
    "To this end, even while writing programs, we can write _other_ programs to help test those programs we write.\n",
    "While this does introduce a bit of a cyclic problem (who tests the programs doing the testing?), in practice the programs doing the testing are much simpler, and so they are less likely to have bugs.\n",
    "Such testing programs can still have bugs, to be clear, but any inconsistency between the program being tested and the program doing the testing will be caught.\n",
    "The end result in practice tends to be much better than doing no testing at all, or manually testing and verifying output by hand.\n",
    "\n",
    "### Background: Unit Testing and Test Suites ###\n",
    "\n",
    "Arguably the most common kind of this sort of automated testing is _unit testing_.\n",
    "The idea with unit testing is to divide code up into individual _units_, and then test those units.\n",
    "Exactly what a \"unit\" is is up to the programmer.\n",
    "It might be as small as a single function, or as big as an entire module, though best practices tend to prefer division into smaller units.\n",
    "(Smaller units tend to be more straghtforward to test.)\n",
    "The only real requirement is that, whatever your \"unit\" is, it has a well-defined input and output.\n",
    "\n",
    "Once you've identified a unit, you can then write tests over that unit.\n",
    "Tests similarly can be as big or as small as you want, though best practices encourage smaller tests, for similar reasons as preferring smaller units.\n",
    "Each test ideally should test one sort of thing your code does, or _behaviors_, allowing you to divide tests up by individual behaviors your code has.\n",
    "Exactly what counts as a \"behavior\" is also largely in the eye of the beholder, but it's common to see different tests testing different parts of the code.\n",
    "For example, if we wanted to test code that contained an `if`, we likely would want at least one test where the `if`'s condition evaluated to `True`, and another test where the `if`'s condition evaluated to `False`.\n",
    "\n",
    "A collection of tests is referred to as a _test suite_.\n",
    "It's common to have one test suite defined per unit, but sometimes you'll see multiple test suites for the same unit (particularly when that one unit can have different whole collections of behaviors associated with it), or conversely no test suites for a given unit (e.g., it's not tested at all, or it is tested in conjunction with another unit).\n",
    "\n",
    "While you can write code to assist with unit testing yourself, customarily we use software libraries for this purpose.\n",
    "Python even comes with a software testing library out of the box: `unittest`.\n",
    "To use `unittest`, you must do the following:\n",
    "\n",
    "- Import the library with `import unittest`.\n",
    "- Define a class which inherits from the `unittest.TestCase` class.  This class forms a test suite.\n",
    "- Define methods on this class.  Each method forms one test.\n",
    "- Inside the methods themselves, call various [assertions](https://docs.python.org/3/library/unittest.html#assert-methods).  An assertion checks one particular thing.  Importantly, an assertion can fail, which will trigger failure of the test the assertion is contained within.  We will specifically cover [`assertEqual`](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertEqual) here, which is one of the most general kinds of assertions.\n",
    "- Run the test suite, by calling the `unittest.main` method with the correct parameters.\n",
    "\n",
    "> As a fair wanring, in contrast to normal Python convention, the assertions seen in `unittest` are in camel case instead of snake case, despite the fact that these assertions are methods.\n",
    "> This is because the `unittest` library is intended to be as close as possible to [Java's JUnit library](https://junit.org/).\n",
    "> By convention, Java uses camel case for its method names, and so `unittest` also went with the camel case method names to match up with the Java assertion names.\n",
    "\n",
    "An example of a unit test suite defined with `unittest` is shown below, which tests an `add` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a035169-ac7a-4fe3-a64c-925c8beb4992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_failure (__main__.TestAdd.test_failure) ... FAIL\n",
      "test_plus_1_2 (__main__.TestAdd.test_plus_1_2) ... ok\n",
      "test_plus_3_4 (__main__.TestAdd.test_plus_3_4) ... ok\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_failure (__main__.TestAdd.test_failure)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\thant\\AppData\\Local\\Temp\\ipykernel_47756\\2509612401.py\", line 16, in test_failure\n",
      "    self.assertEqual(True, False)\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\n",
      "AssertionError: True != False\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.005s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x166c50b1160>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "class TestAdd(unittest.TestCase):\n",
    "    def test_plus_1_2(self):\n",
    "        self.assertEqual(3, add(1, 2))\n",
    "\n",
    "    def test_plus_3_4(self):\n",
    "        self.assertEqual(7, add(3, 4))\n",
    "\n",
    "    def test_failure(self):\n",
    "        # intentionally have a test with a failing assertion,\n",
    "        # for demonstration purposes\n",
    "        self.assertEqual(True, False)\n",
    "        \n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393a13a7-3e10-4bc9-887f-7a3ed760bf82",
   "metadata": {},
   "source": [
    "To explain the code a bit, we start with the `import` of the `unittest` library.\n",
    "From there, we define the `add` function which we wish to test.\n",
    "The test suite itself is named `TestAdd`, which must be defined as a class inheriting from the `unittest.TestCase` class.\n",
    "By convention, test suite names should begin with `Test`.\n",
    "From there, each test is defined with a separate method.\n",
    "The names of these methods **must** begin with `test_`, or else they will not be treated as tests; sometimes you intentionally don't want a method to be treated as a test, in which case it _shouldn't_ begin with `test_`, but for our purposes, we will never see this.\n",
    "The rest of the name of the method should describe in plain English what the test is doing.\n",
    "It's fairly common for tests to have long, descriptive names.\n",
    "From there, each method takes `self`, and only `self`.\n",
    "Inside the methods, we can write any regular old code, but the real workhorse is in the assertions.\n",
    "The `assertEqual` method is inherited from `unittest.TestCase`, and it takes two values.\n",
    "If `assertEqual` is called with two values which are considered equal (with `==`, under the hood), then nothing appears to happen.\n",
    "If, however, `assertEqual` is called with two non-equal values, as it is in `test_failure`, then `assertEqual` will cause the enclosing test (and only the enclosing test) to fail.\n",
    "The final line `unittest.main(argv=[''], verbosity=2, exit=False)` is used to run all test suites defined.\n",
    "Note that because we are running this through Jupyter noteooks, this looks a little weirder than usual.\n",
    "This normally looks something more like the following, which assumes the tests are written as a separate, standalone program:\n",
    "\n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n",
    "```\n",
    "\n",
    "Looking at the output of the code, you get a test-by-test breakdown of what every test did.\n",
    "All the tests pass, except for the `test_failure` test, which is intentionally setup with an assertion of two non-equal values and therefore causes the test to fail.\n",
    "You get some additional information for this failing test, showing exactly where the test fails.\n",
    "\n",
    "### Try this Yourself ###\n",
    "\n",
    "The next cell defines a `my_min` function, which returns whichever of its two inputs is smallest.\n",
    "Using `unittest`, define a test suite containing **three tests**, namely:\n",
    "\n",
    "- One where the first input is less than the second\n",
    "- One where the two inputs are equal to each other\n",
    "- One where the first input is greater than the second\n",
    "\n",
    "Be sure to include the `unittest.main(argv=[''], verbosity=2, exit=False)` at the end of the cell in order to actually run your code.\n",
    "Note that this line will run _all_ test suites which have been defined, not just the one you're defining here, so you will end up seeing test output from the prior cell (assuming you can the prior cell).\n",
    "In the output, be sure to confirm that your three tests are running and passing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f3baf8a-a46c-47f1-b706-ee37465828af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_1stequal2nd (__main__.TestMin.test_1stequal2nd) ... ok\n",
      "test_1stgreater2nd (__main__.TestMin.test_1stgreater2nd) ... ok\n",
      "test_1stlessthan2nd (__main__.TestMin.test_1stlessthan2nd) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.004s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1b6c5f73390>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "def my_min(x, y):\n",
    "    if x < y:\n",
    "        return x\n",
    "    else:\n",
    "        return y\n",
    "\n",
    "# Define your test suite here.\n",
    "# In the output, be sure to confirm that your three tests are running and passing.\n",
    "class TestMin(unittest.TestCase):\n",
    "    def test_1stlessthan2nd(self):\n",
    "        self.assertEqual(1, my_min(1, 2))\n",
    "\n",
    "    def test_1stequal2nd(self):\n",
    "        self.assertEqual(3, my_min(3, 3))\n",
    "\n",
    "    def test_1stgreater2nd(self):\n",
    "        self.assertEqual(3, my_min(4, 3))\n",
    "        \n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1eacb5-a5b8-47b4-bf6a-a8ace53aa383",
   "metadata": {},
   "source": [
    "## Step 2: Define Unit Tests over a `min_list` Function ##\n",
    "\n",
    "This step does not introduce anything new per se, but rather gives you more experience using `unittest` to write test suites.\n",
    "The next cell defines a `min_list` function, which is used to return the smallest value in a given list.\n",
    "`min_list` calls the `my_min` function defined in the prior step.\n",
    "Using `unitest`, Define a test suite over `min_list`, with at least the following tests:\n",
    "\n",
    "- On a list containing one element, `min_list` returns that single element\n",
    "- On a list containing two elements, `min_list` returns whichever is smaller, with the smallest first\n",
    "- On a list containing two elements, `min_list` returns whichever is smaller, with the smallest second\n",
    "- On a list containing three elements, `min_list` returns whichever is smaller.  The smallest value can be in any position in the list you desire.\n",
    "\n",
    "As before, be sure to look at the output to ensure that the tests you wrote are actually running and passing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00d61cf6-8d2c-4992-b748-d264b4be0bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_1element (__main__.TestMin.test_1element) ... ok\n",
      "test_2elements1st (__main__.TestMin.test_2elements1st) ... ok\n",
      "test_2elements2nd (__main__.TestMin.test_2elements2nd) ... ok\n",
      "test_3elements (__main__.TestMin.test_3elements) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.008s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1b6c613fdf0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "def min_list(input_list):\n",
    "    running_min = input_list[0]\n",
    "    for element in input_list:\n",
    "        running_min = my_min(element, running_min)\n",
    "    return running_min\n",
    "\n",
    "# Define your test suite here.\n",
    "# In the output, be sure to confirm that your three tests are running and passing.\n",
    "\n",
    "class TestMin(unittest.TestCase):\n",
    "    def test_1element(self):\n",
    "        self.assertEqual(10, min_list([10]))\n",
    "\n",
    "    def test_2elements1st(self):\n",
    "        self.assertEqual(5, min_list([5, 12]))\n",
    "\n",
    "    def test_2elements2nd(self):\n",
    "         self.assertEqual(8, min_list([15, 8]))\n",
    "\n",
    "    def test_3elements(self):\n",
    "        self.assertEqual(2, min_list([5, 2, 8]))\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8dfaa1-4015-4055-9758-414745e36ce0",
   "metadata": {},
   "source": [
    "## Step 3: Submit via Canvas ##\n",
    "\n",
    "Be sure to **save your work**, then log into [Canvas](https://canvas.csun.edu/).  Go to the COMP 502 course, and click \"Assignments\" on the left pane.  From there, click \"Assignment 21\".  From there, you can upload the `21_automated_testing.ipynb` file.\n",
    "\n",
    "You can turn in the assignment multiple times, but only the last version you submitted will be graded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a66c82-68ed-4bcc-8fee-57c1db8e86dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7600f06-5ee1-4159-b345-9775944e9dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f494a8e8-7806-429e-857f-42b27701c2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
