{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3312867-7fe5-4919-a23b-77f1fd9e6802",
   "metadata": {},
   "source": [
    "# Assignment 30: Pandas Data Cleaning #\n",
    "\n",
    "### Goals for this Assignment ###\n",
    "\n",
    "By the time you have completed this assignment, you should be able to:\n",
    "\n",
    "- Use the `isna` method to determine if a given datum is missing\n",
    "- Use the `dropna` method to remove rows and/or columns containing missing data\n",
    "- Use the `drop_duplicates` method to remove possibly partially-duplicated rows\n",
    "- Use the `replace` method to replace values with other values\n",
    "\n",
    "## Step 1: Use the `isna` Method to Find Missing Data ##\n",
    "\n",
    "### Background: Data Cleaning ###\n",
    "\n",
    "For analysis purposes, a data set should ideally be complete.\n",
    "However, raw data has a tendency to be imperfect, especially if data entry needs to be performed at all manually.\n",
    "For example:\n",
    "\n",
    "- Individual cells might not contain a value.  Exactly why could be for a number of reasons: temporary disconnection from a sensor, a failing data storage, someone just plain forgot to include it, etc.\n",
    "- Whole rows or columns may be missing values, in the same manner as the prior bullet.  Perhaps the data is incomplete, and some experiment was started but never finished.  Perhaps there were some placeholders that were intended to be filled-in later, but we never got around to it, or something happened where they couldn't be filled-in.\n",
    "- Conversely, data may be duplicated in a data set.  For example, if the data was represented originally as a spreadsheet, someone may have been moving data around and accidentally did two pastes after a single copy.  Especially on large data sets, if there is any sort of manual transcription required, it's easy to accidentally redo some work; this is especially true considering that redoing work just means lost time, whereas forgetting to do something means lost data.\n",
    "- There may be contradictory values in the data set which are not internally consistent.  For example, consider a data set containing customer information, including both the city and country the customer resides in.  If the listed city was New York City, but the listed country was Portugal, this would not be internally consistent.\n",
    "- Building on the prior point, there could be dramatic outliers, to the point where they seem more likely to be errors than actual data points.  For example, if a data set records details of how employees commute to the office, it would be more than a little odd if someone says they commute by foot, live 7 miles away, and it takes them 30 minutes to get to work.  This isn't impossible, but the pacing rivals Olympic-level athletes.  It's far more likely that the employee is overreporting distance, underreporting time, or indicated the wrong means of transportation.\n",
    "- There is a strange pattern in the data, where individual data points make sense, but as a collection they don't.  While this could indicate a meaningful pattern, it might also be due to what ultimately amounts to an error.  For example, in many locations it would not be strange to occasionally see a wind speed sensor read 0 mph.  However, a long string of 0 mph readings more likely indicates that the sensor got stuck, especially if you only see 0 mph readings after a certain point.  Faulty sensors can lead to all sorts of strange, bogus readings, and detecting failure may not be straightforward.\n",
    "\n",
    "A data set that is free of these sorts of problems is said to be _clean_.\n",
    "Similarly, the process of transforming a data set to be rid of these problems is referred to as _data cleanup_.\n",
    "In practice, [a significant amount of time is spent on data cleanup](https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/), though the amount of time varies wildly depending on the particular data set (that source, which cites many other sources, ranges from 15% to 90% of the time on data cleanup).\n",
    "A big chunk of this time is spent in understanding what the data is showing, identifying issues, and devising strategies and solutions for fixing these issues.\n",
    "\n",
    "Fixing problems can be as simple as just throwing out entire rows, or perhaps even entire columns, when a problem is found involving a given row or column.\n",
    "However, this could lead to an unacceptable amount of data being discarded.\n",
    "Furthermore, if there is reason to believe that data which would be discarded is somehow different from other data beyond just the issue, this would lead to a systematic bias getting thrown into the analysis.\n",
    "For example, for the situation where the one employee had an Olympic-level running pace, a potential solution is to drop all rows where employees reported they commute by foot.\n",
    "However, this not only discards data, but it systematically discards only employees who commute by foot.\n",
    "For example, if we wanted to determine the average employee distance to the office, discarding everyone who commutes by foot would very likely increase the average distance to the office, perhaps significantly so if many people commute by foot.\n",
    "All this to day: data cleaning is not always straightforward, and we need to be cognizant of the analyses we plan to perform.\n",
    "\n",
    "> From my own experiences, my [MS thesis](https://repository.rit.edu/theses/4078/) dealt with a primarily manually-entered data set.\n",
    "> This data set had originally been represented as an Excel spreadsheet spanning 10 columns and 4,348 rows, and had been edited over the course of several years by a multitude of people.\n",
    "> Missing cells were common, as were internal inconsistencies, along with partial or complete duplications of rows.\n",
    "> I worked with this data set for multiple years and spent approximately one year cleaning it, though in my case I got a bit distracted and built general-purpose tools for data cleanup instead of focusing solely on the task at hand.\n",
    "> Looking back on it now, Pandas would have dramatically simplified this, and would have likely made the programming side of the problem so easy that I couldn't justify building my own tooling.\n",
    "> This effort started in 2010, and Pandas supposedly had been open source for two years at that point, but I didn't know about it and no one I knew was talking about it.\n",
    "> Python similarly was no where near as popular as it is now, and I even knew people who actively discouraged learning it because they thought it was a fad.\n",
    "\n",
    "### Background: Missing Data in Pandas ###\n",
    "\n",
    "In Pandas, missing datums can be represented in different ways depending on the type of the column.\n",
    "[`NaN` (Not a Number)](https://en.wikipedia.org/wiki/NaN) is used for a number of representations, including floating-point values.\n",
    "To see this in action, consider the next cell, which was taken from step 3 of assignment 28.\n",
    "In this case, an outer join of `hardware_inventory` and `hardware_purchases` was made on the `\"product\"` column.\n",
    "However, the product `\"jigsaw\"` in `hardware_inventory` had no corresponding entries in `hardware_purchases`, and similarly the product `\"table saw\"` in `hardware_purchases` had no corresponding entries in `hardware_inventory`.\n",
    "This lead to `NaN` being used in some entries in the table.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b07dc535-3679-46c5-8388-955668c0d395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       product  price  quantity  location customer_name       when\n",
      "0       hammer   20.0      19.0     tools         alice   1/2/2025\n",
      "1       hammer   20.0      19.0     tools           joe   5/6/2025\n",
      "2       jigsaw   40.0      12.0     tools           NaN        NaN\n",
      "3  screws (20)    2.5     150.0  hardware           bob   3/4/2025\n",
      "4    table saw    NaN       NaN       NaN          carl  5/12/2015\n",
      "5       wrench   30.0      15.0     tools         alice   1/2/2025\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hardware_inventory = pd.DataFrame({\"product\": [\"hammer\", \"wrench\", \"screws (20)\", \"jigsaw\"],\n",
    "                                   \"price\": [20, 30, 2.5, 40],\n",
    "                                   \"quantity\": [19, 15, 150, 12],\n",
    "                                   \"location\": [\"tools\", \"tools\", \"hardware\", \"tools\"]})\n",
    "hardware_purchases = pd.DataFrame({\"customer_name\": [\"alice\", \"alice\", \"bob\", \"joe\", \"carl\"],\n",
    "                                   \"product\": [\"hammer\", \"wrench\", \"screws (20)\", \"hammer\", \"table saw\"],\n",
    "                                   \"when\": [\"1/2/2025\", \"1/2/2025\", \"3/4/2025\", \"5/6/2025\", \"5/12/2015\"]})\n",
    "\n",
    "combined_hardware = hardware_inventory.merge(hardware_purchases, on=\"product\", how=\"outer\")\n",
    "print(combined_hardware)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeafa86-965e-46eb-a15b-48a623581d70",
   "metadata": {},
   "source": [
    "In Pandas, the preferred way to check if some data is missing is with the `isna` method on `Series` objects.\n",
    "`isna` yields a new `Series` object of the same length, containing a Boolean value for the entries which were missing.\n",
    "Importantly, `isna` understands all the different representations of missing data for all types in Pandas, not just `NaN`; this means we can apply `isna` to any `Series` object no matter what `dtype` is, and `isna` will work consistently\n",
    "We can see this in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27423c07-4e68-4a10-b853-e83b22c47f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    False\n",
      "1    False\n",
      "2    False\n",
      "3    False\n",
      "4     True\n",
      "5    False\n",
      "Name: price, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(combined_hardware[\"price\"].isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dab258-8207-4415-b6c0-1c3ceff62e55",
   "metadata": {},
   "source": [
    "As shown above, row `4` contains `True`, which follows from the fact that row `4` of the `\"price\"` column contained a `Nan` at this position.\n",
    "In contrast, all other positions had actual values, and therefore `isna` returned `False` for those positions.\n",
    "\n",
    "As another example, let's look at the `\"customer_name\"` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aa80f67-2444-4a8b-838b-f8b0ee6196da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    False\n",
      "1    False\n",
      "2     True\n",
      "3    False\n",
      "4    False\n",
      "5    False\n",
      "Name: customer_name, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(combined_hardware[\"customer_name\"].isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b948702-891b-41f3-af86-ce398779dbaa",
   "metadata": {},
   "source": [
    "Here, row `2` has a `True` which follows from the fact that `\"customer_name\"` has `NaN` at this position.\n",
    "\n",
    "### Background (and Warning): `NaN` is Weird ###\n",
    "\n",
    "You may (rightfully!) think that you could do what `isna` is doing above yourself, at least specifically for checking for `NaN`.\n",
    "For example, it seems like you should be able to use the vector operation `==` with `NaN` to gather the same information (i.e., `some_series == NaN`).\n",
    "Here comes the first weird caveat: to do this comparison, you need an instance of `NaN`, and that isn't as straightforward as you might think.\n",
    "One way to do this is by using `float` to parse the _string_ `\"NaN\"`, as with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "217379fc-8207-4239-9223-d968a487c790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "print(float(\"NaN\")) # prints nan, a floating-point value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00abc22b-4bf0-413b-a194-b4f7eb89a47e",
   "metadata": {},
   "source": [
    "Another way (and the way we will use here, moving forward) is to use `nan`'s definition from NumPy, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b4bf942-5262-4ccc-a755-b13d5f3e3126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b9d518-e6a1-459e-a164-783f2f3f80c3",
   "metadata": {},
   "source": [
    "Now that we have a `NaN` instance, we can look for it as desired.\n",
    "The next cell effectively tries to do `combined_hardware[\"price\"].isna()`, but without using `isna`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8f8adff-9762-486e-bd41-a23954a70e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    False\n",
      "1    False\n",
      "2    False\n",
      "3    False\n",
      "4    False\n",
      "5    False\n",
      "Name: price, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(combined_hardware[\"price\"] == np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7647b5b2-1518-4a9e-9461-f86a91fa3449",
   "metadata": {},
   "source": [
    "Look specifically at row `4`: it's now `False`, not `True`, _even though_ this row contains `NaN`!\n",
    "This weird behavior follows from how equality over `NaN` works, as shown in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "163dce36-613e-4160-bec3-c128cf545dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.nan == np.nan) # prints False\n",
    "print(np.nan != np.nan) # prints True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd29287-12c1-4f63-9831-c0d5328327fc",
   "metadata": {},
   "source": [
    "That is, counter to almost everything imaginable, `NaN` is **not** equal to itself.\n",
    "This is not a Pandas, NumPy, or Python bug, but rather a consequence of the [floating point standard itself](https://en.wikipedia.org/wiki/IEEE_754).\n",
    "That is, the standard itself says that `NaN` is not equal to itself, and anything that follows the standard must follow this same behavior; it'd be a bug to say that `NaN` is equal to itself.\n",
    "\n",
    "Using NumPy, the correct way to check for `NaN` is the `isnan` method, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "729aaef2-3b40-4edc-b111-83607edd1f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(np.isnan(np.nan)) # prints True\n",
    "print(np.isnan(3.14)) # prints False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fa0a2b-1fc5-45e0-84b9-838a37e0f89e",
   "metadata": {},
   "source": [
    "If you are specifically checking for missing data, it is **strongly** recommended to use the `isna` method as opposed to trying to check for `NaN` yourself.\n",
    "Not only will `isna` work correctly for `NaN` without extra work, it will also work for other data types which use representations other than `NaN` to represent missing data.\n",
    "\n",
    "> General warning: if you ever need to check if two floating point numbers are equal, you should be cognizant of this behavior with `NaN`.\n",
    "> Something as innocent-looking as `a == b` won't do what you expect if `a` and `b` are both `NaN`.\n",
    "> This also applies if you have a data structure that contains a floating-point value, and you want to compare that data structure itself to another data structure containing floating-point values.\n",
    "> (I've lost multiple hours of my life to this issue before.)\n",
    "\n",
    "### Try this Yourself ###\n",
    "\n",
    "The next cell defines a `Series` object bound to variable `has_missing` which contains some missing values (with `np.nan`).\n",
    "Use `isna` along with `print` to print out which elements are missing (or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da2518f6-95bd-402b-943b-12ef654f33c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a    False\n",
      "b    False\n",
      "c    False\n",
      "d     True\n",
      "e    False\n",
      "f     True\n",
      "g    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "has_missing = pd.Series([3.14, 1.44, 2.9, np.nan, 9.0, np.nan, 4.5],\n",
    "                        index=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"])\n",
    "\n",
    "# Call and print the result of isna.\n",
    "print(has_missing.isna())\n",
    "\n",
    "#This should print the\n",
    "# following:\n",
    "# a    False\n",
    "# b    False\n",
    "# c    False\n",
    "# d     True\n",
    "# e    False\n",
    "# f     True\n",
    "# g    False\n",
    "# dtype: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0793c837-9baf-46d2-93e8-111e6b423357",
   "metadata": {},
   "source": [
    "## Step 2: Use `dropna` to Remove Rows and/or Columns with Missing Data ##\n",
    "\n",
    "### Background: Logical Not, And, and Or over `Series` Objects ###\n",
    "\n",
    "We've previously seen Python's `not`, `and`, and `or`, which compute logical NOT, AND, and OR, respectively.\n",
    "A quick recap of these operations is in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c42cb3b-8657-4188-94c3-a60f0076ce64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(not True) # prints False\n",
    "print(not False) # prints True\n",
    "\n",
    "print()\n",
    "print(True and True) # prints True\n",
    "print(True and False) # prints False\n",
    "print(False and True) # prints False\n",
    "print(False and False) # prints False\n",
    "\n",
    "print()\n",
    "print(True or True) # prints True\n",
    "print(True or False) # prints True\n",
    "print(False or True) # prints True\n",
    "print(False or False) # prints False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9c69e2-a1da-4299-95f8-341f5a33798a",
   "metadata": {},
   "source": [
    "When it comes to arithmetic operations, both Pandas and Python use the same operators.\n",
    "That is, `+` means addition, `-` means subtraction, and so on.\n",
    "However, this is **not** true for logical operators.\n",
    "For example, let's say we want to compute logical not over a `Series` of Boolean values.\n",
    "Let's see what happens if we try this with `not`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "700e7c2c-7777-418c-8cdb-98a08c6bd81c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31952\\675237403.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m booleans = pd.Series([True, False, False, True],\n\u001b[0;32m      2\u001b[0m                      \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"a\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"b\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"d\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mbooleans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1575\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1576\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1577\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   1578\u001b[0m             \u001b[1;33mf\"\u001b[0m\u001b[1;33mThe truth value of a \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m is ambiguous. \u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1579\u001b[0m             \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1580\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "booleans = pd.Series([True, False, False, True],\n",
    "                     index=[\"a\", \"b\", \"c\", \"d\"])\n",
    "print(not booleans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68bae32-4983-49b4-8e12-8770eec093cb",
   "metadata": {},
   "source": [
    "This throws a `ValueError` exception, saying that the \"true value of a `Series` is ambiguous.\"\n",
    "To explain this error,`not`, along with `and` and `or`, are hardwired into Python itself, and **always** refer to the operations which work strictly over Booleans.\n",
    "Python therefore is attempting to convert `booleans` itself into a single Boolean value, but `Series` objects don't inherently work this way.\n",
    "There _are_ methods on `Series` objects which can give back a Boolean, including `any()` and `all()`, along with the others shown in the error message.\n",
    "In this case, Python (and Pandas) thinks you want a single Boolean value, not a `Series` object holding Boolean values.\n",
    "\n",
    "> The specific feature that Python has which allows operations like `+` to work over `Series` objects is [_operator overloading_](https://en.wikipedia.org/wiki/Operator_overloading).\n",
    "> This is fairly easy to do in Python by defining methods on a custom class with specific names, detailed [in the official Python documentation](https://docs.python.org/3/reference/datamodel.html#special-method-names).\n",
    "> Exactly how to do this is beyond our scope.\n",
    "\n",
    "To apply these logical operations to a `Series` object as a vector operation, we need to use the following:\n",
    "\n",
    "- `~`: logical NOT\n",
    "- `&`: logical AND\n",
    "- `|`: logical OR\n",
    "\n",
    "Examples are shown in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40ec9e11-2a39-4874-9de4-0e2f0fba064f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~booleans1\n",
      "a    False\n",
      "b     True\n",
      "c     True\n",
      "d    False\n",
      "dtype: bool\n",
      "\n",
      "booleans1 & booleans2\n",
      "a     True\n",
      "b    False\n",
      "c    False\n",
      "d    False\n",
      "dtype: bool\n",
      "\n",
      "booleans1 | booleans2\n",
      "a     True\n",
      "b    False\n",
      "c     True\n",
      "d     True\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "booleans1 = pd.Series([True, False, False, True],\n",
    "                      index=[\"a\", \"b\", \"c\", \"d\"])\n",
    "booleans2 = pd.Series([True, False, True, False],\n",
    "                      index=[\"a\", \"b\", \"c\", \"d\"])\n",
    "print(\"~booleans1\")\n",
    "print(~booleans1)\n",
    "\n",
    "print()\n",
    "print(\"booleans1 & booleans2\")\n",
    "print(booleans1 & booleans2)\n",
    "\n",
    "print()\n",
    "print(\"booleans1 | booleans2\")\n",
    "print(booleans1 | booleans2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76d0b06-a16a-44eb-ac98-b3ac8ed81629",
   "metadata": {},
   "source": [
    "### Background: `dropna` ###\n",
    "\n",
    "Using `isna` with masking, you can remove missing data.\n",
    "However, because masking takes rows which are `True`, and `isna` gives `True` for missing values, you need to logically negate the result from `isna`.\n",
    "An example of this is shown in the next cell, using the `combined_hardware` data from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b17feb9-86f0-434e-acdc-efd9205518a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       product  price  quantity  location customer_name      when\n",
      "0       hammer   20.0      19.0     tools         alice  1/2/2025\n",
      "1       hammer   20.0      19.0     tools           joe  5/6/2025\n",
      "2       jigsaw   40.0      12.0     tools           NaN       NaN\n",
      "3  screws (20)    2.5     150.0  hardware           bob  3/4/2025\n",
      "5       wrench   30.0      15.0     tools         alice  1/2/2025\n"
     ]
    }
   ],
   "source": [
    "# copied for convenience\n",
    "hardware_inventory = pd.DataFrame({\"product\": [\"hammer\", \"wrench\", \"screws (20)\", \"jigsaw\"],\n",
    "                                   \"price\": [20, 30, 2.5, 40],\n",
    "                                   \"quantity\": [19, 15, 150, 12],\n",
    "                                   \"location\": [\"tools\", \"tools\", \"hardware\", \"tools\"]})\n",
    "hardware_purchases = pd.DataFrame({\"customer_name\": [\"alice\", \"alice\", \"bob\", \"joe\", \"carl\"],\n",
    "                                   \"product\": [\"hammer\", \"wrench\", \"screws (20)\", \"hammer\", \"table saw\"],\n",
    "                                   \"when\": [\"1/2/2025\", \"1/2/2025\", \"3/4/2025\", \"5/6/2025\", \"5/12/2015\"]})\n",
    "\n",
    "combined_hardware = hardware_inventory.merge(hardware_purchases, on=\"product\", how=\"outer\")\n",
    "print(combined_hardware[~combined_hardware[\"price\"].isna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64175429-47fa-486d-bdc5-d0f746d3e58e",
   "metadata": {},
   "source": [
    "As shown, this ended up removing all rows where the `\"price\"` column was missing.\n",
    "However, row `2` is still missing data, as row `2` nonetheless was not missing `\"price\"`.\n",
    "\n",
    "We could address this issue by adjusting the mask, as with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a98d007-4f44-46e3-86a5-bf34d22aa355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       product  price  quantity  location customer_name      when\n",
      "0       hammer   20.0      19.0     tools         alice  1/2/2025\n",
      "1       hammer   20.0      19.0     tools           joe  5/6/2025\n",
      "3  screws (20)    2.5     150.0  hardware           bob  3/4/2025\n",
      "5       wrench   30.0      15.0     tools         alice  1/2/2025\n"
     ]
    }
   ],
   "source": [
    "print(combined_hardware[~(combined_hardware[\"price\"].isna() | combined_hardware[\"customer_name\"].isna())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa08fc6-49ec-4260-b2a8-ce22681420a7",
   "metadata": {},
   "source": [
    "However, in general, we'd have to do this sort of adjustment over every column of the `DataFrame`.\n",
    "\n",
    "It turns out that this is such a common operation that Pandas offers a much simpler solution, just for this problem: `DataFrame`'s `dropna` method.\n",
    "`dropna` will remove rows and/or columns containing missing values, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47b1cc5d-6f8f-4025-8ec6-90ea7df689e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       product  price  quantity  location customer_name      when\n",
      "0       hammer   20.0      19.0     tools         alice  1/2/2025\n",
      "1       hammer   20.0      19.0     tools           joe  5/6/2025\n",
      "3  screws (20)    2.5     150.0  hardware           bob  3/4/2025\n",
      "5       wrench   30.0      15.0     tools         alice  1/2/2025\n"
     ]
    }
   ],
   "source": [
    "print(combined_hardware.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6c4ef2-8486-403e-be0c-d07b38503a3a",
   "metadata": {},
   "source": [
    "As shown in the prior cell, any rows which contained missing values have been stripped away in the resulting `DataFrame`.\n",
    "This effectively did the same thing as what we had with the mask, but it is much shorter.\n",
    "\n",
    "You can also strip away any columns containing missing values by passing `axis=\"columns\"` as a keyword argument to `dropna`.\n",
    "An example of this with `combined_hardware` is shown in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddd5a4f3-6ec7-403a-bcd8-3eba745ff258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       product  price  quantity  location customer_name       when\n",
      "0       hammer   20.0      19.0     tools         alice   1/2/2025\n",
      "1       hammer   20.0      19.0     tools           joe   5/6/2025\n",
      "2       jigsaw   40.0      12.0     tools           NaN        NaN\n",
      "3  screws (20)    2.5     150.0  hardware           bob   3/4/2025\n",
      "4    table saw    NaN       NaN       NaN          carl  5/12/2015\n",
      "5       wrench   30.0      15.0     tools         alice   1/2/2025\n",
      "\n",
      "       product\n",
      "0       hammer\n",
      "1       hammer\n",
      "2       jigsaw\n",
      "3  screws (20)\n",
      "4    table saw\n",
      "5       wrench\n"
     ]
    }
   ],
   "source": [
    "print(combined_hardware) # refresher of what the original data was\n",
    "print()\n",
    "print(combined_hardware.dropna(axis=\"columns\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b264f1b-254c-4efc-bd48-785e57f05a17",
   "metadata": {},
   "source": [
    "There are some additional parameters that `dropna` can take to customize its behavior, though they are beyond our scope.\n",
    "You may wish to consult the [official Pandas documentation for `dropna`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html) in order to learn more.\n",
    "\n",
    "### Try this Yourself ###\n",
    "\n",
    "The next cell defines a `DataFrame` and binds it to variable `some_missing`.\n",
    "Using `dropna`, initialize variables `without_missing_rows` and `without_missing_columns` to hold `DataFrame` objects based on `some_missing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1bbad08c-c5d0-4bd6-b1fa-71289612d45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   foos  bars  blahs\n",
      "0   3.3   2.2    1.1\n",
      "3   7.0   7.7    4.4\n",
      "4   7.1   2.1    5.5\n",
      "\n",
      "   blahs\n",
      "0    1.1\n",
      "1    2.2\n",
      "2    3.3\n",
      "3    4.4\n",
      "4    5.5\n"
     ]
    }
   ],
   "source": [
    "some_missing = pd.DataFrame({\"foos\": [3.3, 2.1, np.nan, 7.0, 7.1],\n",
    "                             \"bars\": [2.2, np.nan, 7.6, 7.7, 2.1],\n",
    "                             \"blahs\": [1.1, 2.2, 3.3, 4.4, 5.5]})\n",
    "\n",
    "# Write your code here defining without_missing_rows.\n",
    "# This should be a DataFrame where any rows containing missing\n",
    "# information are stripped away.\n",
    "\n",
    "without_missing_rows = some_missing.dropna()\n",
    "\n",
    "print(without_missing_rows)\n",
    "# the above statement should print:\n",
    "#    foos  bars  blahs\n",
    "# 0   3.3   2.2    1.1\n",
    "# 3   7.0   7.7    4.4\n",
    "# 4   7.1   2.1    5.5\n",
    "\n",
    "# Write your code here defining without_missing_columns.\n",
    "# This should be a DataFrame where any columns containing missing\n",
    "# information are stripped away.\n",
    "without_missing_columns = some_missing.dropna(axis=1)\n",
    "print()\n",
    "print(without_missing_columns)\n",
    "# the above statement should print:\n",
    "#    blahs\n",
    "# 0    1.1\n",
    "# 1    2.2\n",
    "# 2    3.3\n",
    "# 3    4.4\n",
    "# 4    5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3465cdea-32aa-4bf4-ac7d-a27d8199c9b0",
   "metadata": {},
   "source": [
    "## Step 3: Use `drop_duplicates` to Remove Possibly Partially-Duplicated Rows ##\n",
    "\n",
    "### Background: (Partial) Duplications in Data ###\n",
    "\n",
    "Sometimes a data set contains duplicate rows, or partially-duplicated rows.\n",
    "For example, consider this variant of the hardware inventory data we have previously seen in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0389211-432d-4f7d-9fd1-632b5e2c3ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       product  price  quantity  location\n",
      "0       hammer   20.0        19     tools\n",
      "1       wrench   30.0        15     tools\n",
      "2  screws (20)    2.5       150  hardware\n",
      "3       jigsaw   40.0        12     tools\n",
      "4       hammer   20.0        19     tools\n",
      "5       jigsaw   35.0        14     tools\n"
     ]
    }
   ],
   "source": [
    "dups_hardware_inventory = pd.DataFrame({\"product\": [\"hammer\", \"wrench\", \"screws (20)\", \"jigsaw\", \"hammer\", \"jigsaw\"],\n",
    "                                        \"price\": [20, 30, 2.5, 40, 20, 35],\n",
    "                                        \"quantity\": [19, 15, 150, 12, 19, 14],\n",
    "                                        \"location\": [\"tools\", \"tools\", \"hardware\", \"tools\", \"tools\", \"tools\"]})\n",
    "print(dups_hardware_inventory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a928b193-380e-44e6-8d32-e950affc63c2",
   "metadata": {},
   "source": [
    "As shown, rows `0` and `4`, other than their numeric index, share completely identical information; these two rows are total duplicates of each other.\n",
    "Rows `3` and `5` have some data in common (namely `\"product\"` and `\"location\"`), though the remaining values are not shared (`\"price\"` and `\"quantity\"`); in this sense, rows `3` and `5` are partially duplicated.\n",
    "\n",
    "For some data sets, partial duplicates or even total duplicates may be expected, and not considered to be inherently indicative of a problem.\n",
    "For example, within this hardware inventory information, we'd expect that multiple rows would share a value in the `\"location\"` column.\n",
    "Of anything, if `\"location\"` was fully unique across rows, that in and of itself would likely indicate a problem, because it would mean that at most one item is stocked per location.\n",
    "As another example, a car dealership may track the year, make, and model of each car on the lot, where each row in the data set represents one vehicle.\n",
    "If the only things tracked are year, make, and model, it would not be out of the realm of possibility to see completely duplicate rows, representing the fact that two or more cars of the same year, make, and model are on the lot.\n",
    "\n",
    "For other data sets, duplicates, even partial duplicates, may indicate a problem.\n",
    "For example, a data set tracking wind speed over time may include a timestamp and a particular reading at that time.\n",
    "If we have only a single wind speed sensor, we would likely not expect to see multiple entries for wind speed at the exact same time, regardless of the values in the rest of the columns.\n",
    "This all illustrates the importance of understanding the data set before you analyze it: you need to make sure it's clean first, and if not, perform corrective actions of some sort.\n",
    "\n",
    "> From my own experiences with my MS thesis, duplications were some of the most challenging issues to fix.\n",
    "> Total duplicates happened more often than you might think, and tended to be clustered together.\n",
    "> What likely had happened was that someone years prior had started to manually rearrange the data and copy/pasted things around, and accidentally pasted the same data multiple times.\n",
    "> Certain partial duplications were the worst, especially when all entries were internally consistent; these occasionally required consultation with domain experts to resolve.\n",
    "\n",
    "### Background: `DataFrame`'s `drop_duplicates` Method ###\n",
    "\n",
    "If you have come to the conclusion that it's appropriate to strip out rows containing duplicate data, Pandas `DataFrame`s offer a method just for this purpose: `drop_duplicates()`.\n",
    "The simplest form takes no parameters, and will return a new `DataFrame` without any completely duplicate rows.\n",
    "This is shown in the next cell with the `dups_hardware_inventory` `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db9865e2-ab6f-45de-bb7a-4d7da28d6fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       product  price  quantity  location\n",
      "0       hammer   20.0        19     tools\n",
      "1       wrench   30.0        15     tools\n",
      "2  screws (20)    2.5       150  hardware\n",
      "3       jigsaw   40.0        12     tools\n",
      "4       hammer   20.0        19     tools\n",
      "5       jigsaw   35.0        14     tools\n",
      "\n",
      "       product  price  quantity  location\n",
      "0       hammer   20.0        19     tools\n",
      "1       wrench   30.0        15     tools\n",
      "2  screws (20)    2.5       150  hardware\n",
      "3       jigsaw   40.0        12     tools\n",
      "5       jigsaw   35.0        14     tools\n"
     ]
    }
   ],
   "source": [
    "print(dups_hardware_inventory) # reminder of what this data is\n",
    "print()\n",
    "print(dups_hardware_inventory.drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4afb37-ad1f-4ce3-9ad7-2caff83064fe",
   "metadata": {},
   "source": [
    "Looking at the output of the prior cell, the resulting `DataFrame` from `drop_duplicates` lacks a second row where `\"hammer\"` is the product, reflecting the fact that the fully duplicated row has been removed.\n",
    "It's also possible to specify which specific `\"hammer\"`-containing row should be preserved; the [official Pandas documentation for `drop_duplicates`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html) has more information.\n",
    "\n",
    "Looking again at the output of the prior cell, the rows containing `\"jigsaw\"` are both still present in the output, reflecting the fact that the data is not fully duplicated acorss the rows.\n",
    "Specificaly, both have a different `\"price\"` and `\"quantity\"` listed.\n",
    "The `drop_duplicates` method can still handle this sort of situation, with an additional keyword parameter: `subset`.\n",
    "The `subset` parameter can take a list of column names, and only those columns will be used to determine if two rows are identical.\n",
    "For example, if we want two rows to be considered identical if their `\"product\"` and `\"location\"` columns are identical, we can do the following in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a07f984-0ccf-4b55-9a0b-f3a47c4eb0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       product  price  quantity  location\n",
      "0       hammer   20.0        19     tools\n",
      "1       wrench   30.0        15     tools\n",
      "2  screws (20)    2.5       150  hardware\n",
      "3       jigsaw   40.0        12     tools\n",
      "4       hammer   20.0        19     tools\n",
      "5       jigsaw   35.0        14     tools\n",
      "\n",
      "       product  price  quantity  location\n",
      "0       hammer   20.0        19     tools\n",
      "1       wrench   30.0        15     tools\n",
      "2  screws (20)    2.5       150  hardware\n",
      "3       jigsaw   40.0        12     tools\n"
     ]
    }
   ],
   "source": [
    "print(dups_hardware_inventory) # reminder of what this data is\n",
    "print()\n",
    "print(dups_hardware_inventory.drop_duplicates(subset=[\"product\", \"location\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c25171-72df-4eb7-9468-1669d3c2ce7f",
   "metadata": {},
   "source": [
    "### Try this Yourself ###\n",
    "\n",
    "The next cell defines a `DataFrame` and binds it to variable `dups_data`.\n",
    "This is intended to represent daily average temperatures in Fahrenheit, recorded from a somewhat faulty weather station somewhere on the planet.\n",
    "Using `drop_duplicates`:\n",
    "\n",
    "1. Initialize the `without_dup_rows` variable to be `dups_data` without the fully duplicate rows.\n",
    "2. Initialize the `without_dup_when` variable to be `dups_data` without duplicated `\"when\"` information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c89e3bb-f053-4246-ba45-c41f064373e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       when  temp\n",
      "0  1/1/2015    25\n",
      "1  1/2/2015    23\n",
      "2  1/2/2015    23\n",
      "3  1/3/2015    30\n",
      "4  1/4/2015    40\n",
      "5  1/5/2015    21\n",
      "6  1/5/2015    15\n",
      "\n",
      "       when  temp\n",
      "0  1/1/2015    25\n",
      "1  1/2/2015    23\n",
      "3  1/3/2015    30\n",
      "4  1/4/2015    40\n",
      "5  1/5/2015    21\n",
      "6  1/5/2015    15\n",
      "\n",
      "       when  temp\n",
      "0  1/1/2015    25\n",
      "1  1/2/2015    23\n",
      "3  1/3/2015    30\n",
      "4  1/4/2015    40\n",
      "5  1/5/2015    21\n"
     ]
    }
   ],
   "source": [
    "dups_data = pd.DataFrame({\"when\": [\"1/1/2015\", \"1/2/2015\", \"1/2/2015\", \"1/3/2015\", \"1/4/2015\", \"1/5/2015\", \"1/5/2015\"],\n",
    "                          \"temp\": [25, 23, 23, 30, 40, 21, 15]})\n",
    "print(dups_data)\n",
    "\n",
    "print()\n",
    "# Define your code here to initialize without_dup_rows.\n",
    "without_dup_rows = dups_data.drop_duplicates()\n",
    "print(without_dup_rows)\n",
    "# The above statement should print:\n",
    "#        when  temp\n",
    "# 0  1/1/2015    25\n",
    "# 1  1/2/2015    23\n",
    "# 3  1/3/2015    30\n",
    "# 4  1/4/2015    40\n",
    "# 5  1/5/2015    21\n",
    "# 6  1/5/2015    15\n",
    "\n",
    "print()\n",
    "# Define your code here to initialize without_dup_when.\n",
    "without_dup_when = dups_data.drop_duplicates(subset=[\"when\"])\n",
    "\n",
    "print(without_dup_when)\n",
    "# The above statement should print:\n",
    "#        when  temp\n",
    "# 0  1/1/2015    25\n",
    "# 1  1/2/2015    23\n",
    "# 3  1/3/2015    30\n",
    "# 4  1/4/2015    40\n",
    "# 5  1/5/2015    21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83022ce7-ecb6-4c2b-8f93-cbe76706ae76",
   "metadata": {},
   "source": [
    "## Step 4: Use `DataFrame`'s `replace` Method to Replace Fixed Values with Other Fixed Values ##\n",
    "\n",
    "### Background: `replace` Method ###\n",
    "\n",
    "Consider a modified version of the `dups_data` `DataFrame` from the prior step, shown in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a215e9cc-4ca6-4847-9d28-55f09a559119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       when  temp\n",
      "0  1/1/2015    25\n",
      "1  1/2/2015    23\n",
      "2  1/3/2015    23\n",
      "3  1/4/2015    30\n",
      "4  1/5/2015    40\n",
      "5  1/6/2015  9999\n",
      "6  1/7/2015    15\n",
      "7  1/8/2015    18\n",
      "8  1/9/2015  9998\n"
     ]
    }
   ],
   "source": [
    "weather = pd.DataFrame({\"when\": [\"1/1/2015\", \"1/2/2015\", \"1/3/2015\", \"1/4/2015\", \"1/5/2015\", \"1/6/2015\", \"1/7/2015\", \"1/8/2015\", \"1/9/2015\"],\n",
    "                        \"temp\": [25, 23, 23, 30, 40, 9999, 15, 18, 9998]})\n",
    "print(weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d3bd77-fabb-4fd0-af82-76600c16492e",
   "metadata": {},
   "source": [
    "As shown, at rows `5` and `8`, we have recorded temperatures of `9999` and `9998`, respectively.\n",
    "Given that this is supposed to be data from a terrestial weather station, these numbers are impossibly high and must be incorrect.\n",
    "Morever, these numbers don't appear to be random, given how close they are to 1,000, an even power of 10.\n",
    "This suggests that these numbers were very intentionally put here, and were intended to stand out.\n",
    "While we don't know exactly what these numbers mean without further context (perhaps specific error codes?), we can be sure that these are **not** temperatures.\n",
    "With this in mind, `\"temp\"` for these rows is effectively a missing value.\n",
    "However, unlike with `NaN`, these are nonetheless valid numbers, and Pandas has no understanding of what any of this data means.\n",
    "For example, Pandas will still happily compute the mean of `\"temp\"`, if you ask for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce906f15-f405-466b-8d4d-424cc8440563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2241.222222222222\n"
     ]
    }
   ],
   "source": [
    "print(weather[\"temp\"].mean()) # prints 2241.222222222222"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ea9ed7-e62d-4b91-94b4-549b90fe44ea",
   "metadata": {},
   "source": [
    "...though the result is pretty useless, because we can confidently say that the average temperature over the week was **not** in excess of 2,000 degrees Fahrenheit.\n",
    "\n",
    "This is another form of invalid data slipping in, though in this case it's more problematic than just missing data: it's missing data that, at a quick glance, looks like real data.\n",
    "This is something else we need to clean.\n",
    "Towards that end, `Pandas` `DataFrame`s offer a `replace` method, which takes a list of values to replace, along with a corresponding list of values to replace them with.\n",
    "For example, let's say we want to replace both `9998` and `9999` with `NaN`, and truly make them missing values.\n",
    "This is shown in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6142a0fc-1600-4260-bce6-84aa9499ce10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       when  temp\n",
      "0  1/1/2015  25.0\n",
      "1  1/2/2015  23.0\n",
      "2  1/3/2015  23.0\n",
      "3  1/4/2015  30.0\n",
      "4  1/5/2015  40.0\n",
      "5  1/6/2015   NaN\n",
      "6  1/7/2015  15.0\n",
      "7  1/8/2015  18.0\n",
      "8  1/9/2015   NaN\n"
     ]
    }
   ],
   "source": [
    "print(weather.replace([9998, 9999], [np.nan, np.nan]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a54abba-795b-48cf-843c-59a291620079",
   "metadata": {},
   "source": [
    "From here, we can use `dropna` as before to get rid of these rows entirely, as with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "717ceab7-7c0e-4e2e-a2c8-e71805dffdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       when  temp\n",
      "0  1/1/2015  25.0\n",
      "1  1/2/2015  23.0\n",
      "2  1/3/2015  23.0\n",
      "3  1/4/2015  30.0\n",
      "4  1/5/2015  40.0\n",
      "6  1/7/2015  15.0\n",
      "7  1/8/2015  18.0\n"
     ]
    }
   ],
   "source": [
    "print(weather.replace([9998, 9999], [np.nan, np.nan]).dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62da6e6-540f-44c7-8c3b-7364e4cbcf3f",
   "metadata": {},
   "source": [
    "Alternatively, perhaps we want to replace `9999` with `30`, and `9998` with `25`.\n",
    "This would look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a11302b5-54d8-4054-adf3-f32b13dbf175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       when  temp\n",
      "0  1/1/2015    25\n",
      "1  1/2/2015    23\n",
      "2  1/3/2015    23\n",
      "3  1/4/2015    30\n",
      "4  1/5/2015    40\n",
      "5  1/6/2015    30\n",
      "6  1/7/2015    15\n",
      "7  1/8/2015    18\n",
      "8  1/9/2015    25\n"
     ]
    }
   ],
   "source": [
    "print(weather.replace([9998, 9999], [25, 30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d62f91-0ebe-4cf8-aacf-f03edbcf6aa4",
   "metadata": {},
   "source": [
    "We are only scratching the surface of the possibilities with `replace`; you may wish to look at the [offical Pandas documentation for `replace`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html) for more information.\n",
    "\n",
    "### Try this Yourself ###\n",
    "\n",
    "In the next cell, a variant of the `some_missing` `DataFrame` is defined, and bound to the variable `new_some_missing`.\n",
    "In this case, instead of using `np.nan` for missing data, the values `400` and `401` were used.\n",
    "Using `replace`, create a new `DataFrame` where all instances of `400` are replaced with `3.5`, and all instances of `401` are replaced with `4.1`.\n",
    "Bind your resulting `DataFrame` to the `replaced` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96eb36a9-de9f-45cc-88f6-a4adc5fdef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    foos   bars  blahs\n",
      "0    3.3    2.2    1.1\n",
      "1    2.1  401.0    2.2\n",
      "2  400.0    7.6    3.3\n",
      "3    7.0    7.7    4.4\n",
      "4    7.1    2.1  400.0\n",
      "\n",
      "   foos  bars  blahs\n",
      "0   3.3   2.2    1.1\n",
      "1   2.1   4.1    2.2\n",
      "2   3.5   7.6    3.3\n",
      "3   7.0   7.7    4.4\n",
      "4   7.1   2.1    3.5\n"
     ]
    }
   ],
   "source": [
    "new_some_missing = pd.DataFrame({\"foos\": [3.3, 2.1, 400, 7.0, 7.1],\n",
    "                                 \"bars\": [2.2, 401, 7.6, 7.7, 2.1],\n",
    "                                 \"blahs\": [1.1, 2.2, 3.3, 4.4, 400]})\n",
    "print(new_some_missing)\n",
    "print()\n",
    "\n",
    "# Define your code here to define and initialize variable replaced\n",
    "\n",
    "replaced = new_some_missing.replace(to_replace={400: 3.5, 401: 4.1})\n",
    "\n",
    "print(replaced)\n",
    "# The above statement should print:\n",
    "#    foos  bars  blahs\n",
    "# 0   3.3   2.2    1.1\n",
    "# 1   2.1   4.1    2.2\n",
    "# 2   3.5   7.6    3.3\n",
    "# 3   7.0   7.7    4.4\n",
    "# 4   7.1   2.1    3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d04b0c0-2f0c-45be-9d2f-46732009761b",
   "metadata": {},
   "source": [
    "## Step 5: Submit via Canvas ##\n",
    "\n",
    "Be sure to **save your work**, then log into [Canvas](https://canvas.csun.edu/).  Go to the COMP 502 course, and click \"Assignments\" on the left pane.  From there, click \"Assignment 30\".  From there, you can upload your `30_pandas_data_cleaning.ipynb` file.\n",
    "\n",
    "You can turn in the assignment multiple times, but only the last version you submitted will be graded."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
